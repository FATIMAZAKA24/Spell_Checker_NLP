{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5de816edd5f34960ae9ad653c88fdca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eca2afd7e5ed4b59b136f81425f766dc",
              "IPY_MODEL_18b3b23ba05442489df167e9f9381a7d",
              "IPY_MODEL_2634f29b2c2e446ab5c91cf5f18749a7"
            ],
            "layout": "IPY_MODEL_15387d3cd94845eb909af2b2c34a9480"
          }
        },
        "eca2afd7e5ed4b59b136f81425f766dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextareaModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextareaModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextareaView",
            "continuous_update": true,
            "description": "Input:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_cdfc5fbe58634c9184a053a493cebab4",
            "placeholder": "Type a sentence with spelling mistakes",
            "rows": null,
            "style": "IPY_MODEL_027fe51722fa4afea7c581a2fba36d3d",
            "value": "Their is a probleem with this sentance."
          }
        },
        "18b3b23ba05442489df167e9f9381a7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Correct Spelling",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_94398afe99094efeb721ef368428530b",
            "style": "IPY_MODEL_b6f774c61dcb4317ab0a10a79b97284c",
            "tooltip": ""
          }
        },
        "2634f29b2c2e446ab5c91cf5f18749a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextareaModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextareaModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextareaView",
            "continuous_update": true,
            "description": "Corrected:",
            "description_tooltip": null,
            "disabled": true,
            "layout": "IPY_MODEL_0177cd24c466442e8c30cd34ce8ecc42",
            "placeholder": "​",
            "rows": null,
            "style": "IPY_MODEL_0236dea914e84ec1909dda0f6b190488",
            "value": "Their is a problem with this sentence."
          }
        },
        "15387d3cd94845eb909af2b2c34a9480": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdfc5fbe58634c9184a053a493cebab4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "100px",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "600px"
          }
        },
        "027fe51722fa4afea7c581a2fba36d3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94398afe99094efeb721ef368428530b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6f774c61dcb4317ab0a10a79b97284c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "0177cd24c466442e8c30cd34ce8ecc42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "100px",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "600px"
          }
        },
        "0236dea914e84ec1909dda0f6b190488": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import os\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"bittlingmayer/spelling\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "corpus_filepath = os.path.join(path, \"big.txt\")\n",
        "pickle_filepath='/kaggle/working/word_counts.pkl' #to save the word_counts dictionary(our frequency map) so we don't process the corpus everytime\n",
        "\n",
        "\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "import os # To check if the file exists\n",
        "import pickle # To save/load the Python object\n",
        "import string # To get the alphabet easily\n",
        "\n",
        "#Loading and preprocessing the corpus\n",
        "def process_corpus(filepath='big.txt'):   #reading the corpus line by line, lowercasing, applying regular expressions, extracting words, counting frequncy, updating word_count, saving in pkl file, loading word counts, showing top 10 most common words and showing frequency of the and spelling\n",
        "  \"\"\"\n",
        "  Reads a text file, processes it into lowercase words,\n",
        "  \"\"\"\n",
        "  word_counts = Counter()\n",
        "  try:\n",
        "    with open(filepath, 'r', encoding='utf-8') as file:\n",
        "      for line in file:\n",
        "        words_in_line = re.findall(r'\\b[a-zA-Z]+\\b', line.lower())\n",
        "        word_counts.update(words_in_line)\n",
        "    print(f\"Successfully processed the corpus.\")\n",
        "    print(f\"Total words counted (including duplicates): {sum(word_counts.values())}\")\n",
        "    print(f\"Unique words in vocabulary: {len(word_counts)}\")\n",
        "  except FileNotFoundError:\n",
        "    print(f\"Error: File not found at '{filepath}'.\")\n",
        "    print(\"Please ensure 'big.txt' is in the correct directory or update the path.\")\n",
        "    return Counter()\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "    return Counter()\n",
        "  return word_counts\n",
        "    # word_counts is a dictionary(a counter object which is a type of dictionaries in python)\n",
        "    #keys are words , values are the number of times each word appears\n",
        "\n",
        "\n",
        "def save_word_counts(word_counts, pickle_path):\n",
        "  \"\"\"Saves the word_counts Counter object to a pickle file.\"\"\"\n",
        "  print(f\"Saving word counts to {pickle_path}...\")\n",
        "  try:\n",
        "    with open(pickle_path, 'wb') as file:\n",
        "      pickle.dump(word_counts, file)\n",
        "    print(\"Word counts saved successfully.\")\n",
        "  except Exception as e:\n",
        "    print(f\"Error saving word counts: {e}\")\n",
        "\n",
        "\n",
        "def load_word_counts(pickle_path):\n",
        "  \"\"\"Loads word counts from a pickle file.\"\"\"\n",
        "  print(f\"Attempting to load word counts from {pickle_path}...\")\n",
        "  try:\n",
        "    with open(pickle_path, 'rb') as file:\n",
        "      word_counts = pickle.load(file)\n",
        "    print(\"Word counts loaded successfully from pickle file.\")\n",
        "    print(f\"Unique words in loaded vocabulary: {len(word_counts)}\")\n",
        "    return word_counts\n",
        "  except FileNotFoundError:\n",
        "    print(\"Pickle file not found.\")\n",
        "    return None # Indicate failure\n",
        "  except Exception as e:\n",
        "    print(f\"Error loading word counts: {e}\")\n",
        "    return None # Indicate failure\n",
        "\n",
        "\n",
        "WORD_COUNTS = None\n",
        "if WORD_COUNTS is None:\n",
        "  print(\"Processing the original corpus file...\")\n",
        "  WORD_COUNTS = process_corpus(corpus_filepath)\n",
        "\n",
        "\n",
        "  if WORD_COUNTS:\n",
        "    save_word_counts(WORD_COUNTS, pickle_filepath)\n",
        "  else:\n",
        "    print(\"Failed to process the corpus. Cannot save.\")\n",
        "\n",
        "if WORD_COUNTS:\n",
        "  print(\"\\n--- Sample Word Counts (from loaded or processed data) ---\")\n",
        "  # Display the 10 most common words\n",
        "  print(\"Most common words:\", WORD_COUNTS.most_common(10))\n",
        "  # Check the count of a specific word\n",
        "  print(\"Count for 'the':\", WORD_COUNTS['the'])\n",
        "  print(\"Count for 'spelling':\", WORD_COUNTS['spelling'])\n",
        "else:\n",
        "  print(\"\\nCould not load or process word counts. Cannot proceed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEXkjRXkn5th",
        "outputId": "992fe72a-3629-43d8-9ec1-72edb4519565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/spelling\n",
            "Processing the original corpus file...\n",
            "Successfully processed the corpus.\n",
            "Total words counted (including duplicates): 1100811\n",
            "Unique words in vocabulary: 29056\n",
            "Saving word counts to /kaggle/working/word_counts.pkl...\n",
            "Error saving word counts: [Errno 2] No such file or directory: '/kaggle/working/word_counts.pkl'\n",
            "\n",
            "--- Sample Word Counts (from loaded or processed data) ---\n",
            "Most common words: [('the', 79809), ('of', 40024), ('and', 38312), ('to', 28765), ('in', 22023), ('a', 21124), ('that', 12512), ('he', 12401), ('was', 11410), ('it', 10681)]\n",
            "Count for 'the': 79809\n",
            "Count for 'spelling': 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j4bNpFxsp3_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generating Candidate Corrections (Edit Distance 1 : Levenshtein distance)\n",
        "def edits1(word):\n",
        "  \"\"\"\n",
        "  Generates all possible strings that are one edit away from 'word'.\n",
        "  Args:\n",
        "    word (str): The input word.\n",
        "\n",
        "  Returns:\n",
        "    set: A set of all unique strings one edit away from 'word'.\n",
        "  \"\"\"\n",
        "  letters    = string.ascii_lowercase\n",
        "  splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "\n",
        "  # Deletes: Remove one character from the right part\n",
        "  deletes    = [L + R[1:]               for L, R in splits if R] # Check 'if R' ensures right part isn't empty\n",
        "\n",
        "  # Transposes: Swap adjacent characters in the right part (if long enough)\n",
        "  transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1] # Need at least 2 chars to swap\n",
        "\n",
        "  # Replaces: Replace the first char of the right part with any letter\n",
        "  replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters] # Check 'if R'\n",
        "\n",
        "  # Inserts: Insert any letter between the left and right parts\n",
        "  inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "\n",
        "  # Combine all edits into a set to get unique candidates\n",
        "  return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def known(words_to_check, vocabulary):\n",
        "  \"\"\"\n",
        "  Filters a collection of words, returning only those present in the vocabulary.\n",
        "  Returns:\n",
        "    set: A subset of `words_to_check` containing only words found in the vocabulary.\n",
        "  \"\"\"\n",
        "  return set(w for w in words_to_check if w in vocabulary)\n",
        "\n",
        "\n",
        "\n",
        "def suggest_correction_v1(word, vocabulary):\n",
        "    \"\"\"\n",
        "    Suggests the most probable spelling correction for `word` based on edit distance 1.\n",
        "\n",
        "    Returns:\n",
        "        str: The suggested correction (most frequent known word at edit distance 1),\n",
        "             or the original word if it's known or no corrections are found.\n",
        "    \"\"\"\n",
        "    # Normalize the input word to lowercase\n",
        "    word = word.lower()\n",
        "\n",
        "    # 1. Check if the word itself is correct in the vocabulary\n",
        "    if word in vocabulary:\n",
        "        print(f\"'{word}' is already in the vocabulary.\")\n",
        "        return word\n",
        "\n",
        "    # 2. Generate all possible words with one edit distance.\n",
        "    candidates_edits1 = edits1(word)\n",
        "    print(f\"Generated {len(candidates_edits1)} candidates at edit distance 1 for '{word}'.\")\n",
        "\n",
        "    # 3. Filter these candidates to find those that are known words in our vocabulary.\n",
        "    known_candidates = known(candidates_edits1, vocabulary)\n",
        "    print(f\"Found {len(known_candidates)} known candidates: {known_candidates or '{}'}\")\n",
        "\n",
        "    # 4. If there are known candidates, choose the one with the highest frequency.\n",
        "    if known_candidates:\n",
        "        best_candidate = max(known_candidates, key=vocabulary.get)\n",
        "        print(f\"Best candidate based on frequency: '{best_candidate}' (Freq: {vocabulary.get(best_candidate)})\")\n",
        "        return best_candidate\n",
        "\n",
        "    return word  # If no corrections are found, return the original word\n",
        "\n",
        "\n",
        "\n",
        "if WORD_COUNTS:\n",
        "  print(\"\\n--- Testing Spell Correction (Edit Distance 1) ---\")\n",
        "  test_words = [\"speling\", \"babi\", \"hapy\", \"korrectud\", \"thaa\", \"receeve\", \"apple\",\"inglis\"]\n",
        "\n",
        "  for test_word in test_words:\n",
        "    correction = suggest_correction_v1(test_word, WORD_COUNTS)\n",
        "    print(f\"Original: '{test_word}', Suggested: '{correction}'\")\n",
        "    print(\"-\" * 20)\n",
        "else:\n",
        "  print(\"\\nCannot run spell correction tests as WORD_COUNTS is not available.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPV1EhJLn5v5",
        "outputId": "f1c21f57-8891-4ce1-d959-8c6bb14f4d93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing Spell Correction (Edit Distance 1) ---\n",
            "Generated 390 candidates at edit distance 1 for 'speling'.\n",
            "Found 1 known candidates: {'spelling'}\n",
            "Best candidate based on frequency: 'spelling' (Freq: 4)\n",
            "Original: 'speling', Suggested: 'spelling'\n",
            "--------------------\n",
            "Generated 234 candidates at edit distance 1 for 'babi'.\n",
            "Found 2 known candidates: {'babe', 'baby'}\n",
            "Best candidate based on frequency: 'baby' (Freq: 45)\n",
            "Original: 'babi', Suggested: 'baby'\n",
            "--------------------\n",
            "Generated 234 candidates at edit distance 1 for 'hapy'.\n",
            "Found 2 known candidates: {'hay', 'happy'}\n",
            "Best candidate based on frequency: 'happy' (Freq: 218)\n",
            "Original: 'hapy', Suggested: 'happy'\n",
            "--------------------\n",
            "Generated 492 candidates at edit distance 1 for 'korrectud'.\n",
            "Found 0 known candidates: {}\n",
            "Original: 'korrectud', Suggested: 'korrectud'\n",
            "--------------------\n",
            "Generated 232 candidates at edit distance 1 for 'thaa'.\n",
            "Found 3 known candidates: {'thaw', 'than', 'that'}\n",
            "Best candidate based on frequency: 'that' (Freq: 12512)\n",
            "Original: 'thaa', Suggested: 'that'\n",
            "--------------------\n",
            "Generated 388 candidates at edit distance 1 for 'receeve'.\n",
            "Found 1 known candidates: {'receive'}\n",
            "Best candidate based on frequency: 'receive' (Freq: 95)\n",
            "Original: 'receeve', Suggested: 'receive'\n",
            "--------------------\n",
            "'apple' is already in the vocabulary.\n",
            "Original: 'apple', Suggested: 'apple'\n",
            "--------------------\n",
            "Generated 338 candidates at edit distance 1 for 'inglis'.\n",
            "Found 0 known candidates: {}\n",
            "Original: 'inglis', Suggested: 'inglis'\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we can notice that this approach can handle one typo ( given that the word is in our corpus) more than one typo like(korrectud) doesnt work. Which leads us to the approach of check distance2.\n",
        "#Generating Candidate Corrections (Edit Distance 2 : Levenshtein distance)\n",
        "\n",
        "def edits2(word):\n",
        "  # Apply edits1 to each result of edits1(word)\n",
        "  return set(e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "\n",
        "def suggest_correction_v2(word, vocabulary):\n",
        "  \"\"\"\n",
        "  Returns:\n",
        "    str: The suggested correction, prioritizing lower edit distance and then\n",
        "         highest frequency. Returns the original word if no correction found.\n",
        "  \"\"\"\n",
        "  word = word.lower()\n",
        "\n",
        "  #Priority 1: the word in vocab\n",
        "  if word in vocabulary:\n",
        "    return word\n",
        "\n",
        "  # Priority 2: Candidates at Edit Distance 1\n",
        "  candidates_edits1 = edits1(word)\n",
        "  known_candidates_edits1 = known(candidates_edits1, vocabulary)\n",
        "  if known_candidates_edits1:\n",
        "    best_candidate = max(known_candidates_edits1, key=vocabulary.get)\n",
        "    return best_candidate\n",
        "\n",
        "  # Priority 3: Candidates at Edit Distance 2\n",
        "  candidates_edits2 = edits2(word)\n",
        "  known_candidates_edits2 = known(candidates_edits2, vocabulary)\n",
        "\n",
        "  if known_candidates_edits2:\n",
        "    best_candidate = max(known_candidates_edits2, key=vocabulary.get)\n",
        "    return best_candidate\n",
        "\n",
        "  # Priority 4: No known correction found\n",
        "  return word\n",
        "\n",
        "\n",
        "\n",
        "if WORD_COUNTS:\n",
        "  print(\"\\n--- Testing Spell Correction (Edit Distance 1 & 2) ---\")\n",
        "  # Include words that might need 2 edits, and some previous ones\n",
        "  test_words = [\"speling\",\n",
        "                \"korrectud\",\n",
        "                \"hapy\",\n",
        "                \"algorithim\",\n",
        "                \"receeve\",\n",
        "                \"appel\",\n",
        "                \"language\",\n",
        "                \"compromissed\",\n",
        "                \"activiti\",\n",
        "                \"inglis\",\n",
        "               ]\n",
        "\n",
        "  for test_word in test_words:\n",
        "    print(f\"Original: '{test_word}'\")\n",
        "    correction = suggest_correction_v2(test_word, WORD_COUNTS)\n",
        "    print(f\"Suggested: '{correction}'\")\n",
        "    print(\"-\" * 20)\n",
        "else:\n",
        "  print(\"\\nCannot run spell correction tests as WORD_COUNTS is not available.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def tokenize_sentence(sentence):\n",
        "  \"\"\"\n",
        "  Splits a sentence into words and punctuation/whitespace sequences.\n",
        "  Example: \"Helo worl!\" -> ['Helo', ' ', 'worl', '!']\n",
        "  \"\"\"\n",
        "  # This regex finds sequences of letters OR sequences of non-letters\n",
        "  return re.findall(r'[a-zA-Z]+|[^a-zA-Z\\s]+|\\s+', sentence)\n",
        "\n",
        "\n",
        "#Full sentence-level corrections\n",
        "def correct_sentence_simple(sentence, vocabulary):\n",
        "  \"\"\"\n",
        "  Corrects a sentence by applying suggest_correction_v2 to each word.\n",
        "\n",
        "  Returns:\n",
        "    str: The corrected sentence.\n",
        "  \"\"\"\n",
        "  tokens = tokenize_sentence(sentence)\n",
        "  corrected_tokens = []\n",
        "\n",
        "  for token in tokens:\n",
        "    if re.match(r'[a-zA-Z]+', token):\n",
        "      original_case = token\n",
        "      correction = suggest_correction_v2(token.lower(), vocabulary)\n",
        "\n",
        "\n",
        "      if original_case.istitle() and correction:\n",
        "          correction = correction.title()\n",
        "      elif original_case.isupper() and len(original_case) > 1 and correction: # Avoid capitalizing single-letter words like 'a'\n",
        "           # Check if correction is not empty\n",
        "           correction = correction.upper()\n",
        "      # Otherwise, keep the correction lowercase (or as returned)\n",
        "\n",
        "      corrected_tokens.append(correction)\n",
        "    else:\n",
        "      # If it's not a word (punctuation, space), keep it as is\n",
        "      corrected_tokens.append(token)\n",
        "\n",
        "  # Join the tokens back together\n",
        "  return \"\".join(corrected_tokens)\n",
        "\n",
        "\n",
        "if WORD_COUNTS:\n",
        "  print(\"\\n--- Testing Sentence Correction ---\")\n",
        "  test_sentences = [\n",
        "      \"Thiss sentense has somme spelling mistaks.\",\n",
        "      \"Wat is the korrectud anser?\",\n",
        "      \"I receeved the mesage.\",\n",
        "      \"All is well.\",\n",
        "      \"ACTIVITI level is high.\",\n",
        "      \"It was hear.\",\n",
        "      \" i am so hapgy\"\n",
        "  ]\n",
        "\n",
        "  for sentence in test_sentences:\n",
        "    corrected = correct_sentence_simple(sentence, WORD_COUNTS)\n",
        "    print(f\"Original:  '{sentence}'\")\n",
        "    print(f\"Corrected: '{corrected}'\")\n",
        "    print(\"-\" * 30)\n",
        "else:\n",
        "  print(\"\\nCannot run sentence correction tests as WORD_COUNTS is not available.\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-KvasfAn5yE",
        "outputId": "f1403013-4f99-4dd5-d923-5c6f99518547"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing Spell Correction (Edit Distance 1 & 2) ---\n",
            "Original: 'speling'\n",
            "Suggested: 'spelling'\n",
            "--------------------\n",
            "Original: 'korrectud'\n",
            "Suggested: 'corrected'\n",
            "--------------------\n",
            "Original: 'hapy'\n",
            "Suggested: 'happy'\n",
            "--------------------\n",
            "Original: 'algorithim'\n",
            "Suggested: 'algorithim'\n",
            "--------------------\n",
            "Original: 'receeve'\n",
            "Suggested: 'receive'\n",
            "--------------------\n",
            "Original: 'appel'\n",
            "Suggested: 'appeal'\n",
            "--------------------\n",
            "Original: 'language'\n",
            "Suggested: 'language'\n",
            "--------------------\n",
            "Original: 'compromissed'\n",
            "Suggested: 'compromised'\n",
            "--------------------\n",
            "Original: 'activiti'\n",
            "Suggested: 'activity'\n",
            "--------------------\n",
            "Original: 'inglis'\n",
            "Suggested: 'english'\n",
            "--------------------\n",
            "\n",
            "--- Testing Sentence Correction ---\n",
            "Original:  'Thiss sentense has somme spelling mistaks.'\n",
            "Corrected: 'This sentence has some spelling mistake.'\n",
            "------------------------------\n",
            "Original:  'Wat is the korrectud anser?'\n",
            "Corrected: 'Wat is the corrected answer?'\n",
            "------------------------------\n",
            "Original:  'I receeved the mesage.'\n",
            "Corrected: 'I received the message.'\n",
            "------------------------------\n",
            "Original:  'All is well.'\n",
            "Corrected: 'All is well.'\n",
            "------------------------------\n",
            "Original:  'ACTIVITI level is high.'\n",
            "Corrected: 'ACTIVITY level is high.'\n",
            "------------------------------\n",
            "Original:  'It was hear.'\n",
            "Corrected: 'It was hear.'\n",
            "------------------------------\n",
            "Original:  ' i am so hapgy'\n",
            "Corrected: ' i am so happy'\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W407_Uw2n50T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#N-GRAM¶\n",
        "#Generating Bigram counts\n",
        "\n",
        "def process_corpus_ngrams(filepath='big.txt'):\n",
        "  \"\"\"\n",
        "  Reads a text file, processes it into lowercase words, and returns\n",
        "  frequency counts for both unigrams (words) and bigrams (word pairs).\n",
        "\n",
        "  Returns:\n",
        "    tuple: (unigram_counts, bigram_counts)\n",
        "           unigram_counts: collections.Counter {word: count}\n",
        "           bigram_counts: collections.defaultdict(Counter) {prev_word: {word: count}}\n",
        "           Returns (None, None) if file processing fails.\n",
        "  \"\"\"\n",
        "  print(f\"Attempting to read corpus file for N-grams: {filepath}\")\n",
        "  unigram_counts = Counter()\n",
        "\n",
        "  bigram_counts = defaultdict(Counter)\n",
        "  total_words = 0\n",
        "  processed_words = 0\n",
        "\n",
        "  try:\n",
        "    with open(filepath, 'r', encoding='utf-8') as file:\n",
        "      prev_word = None\n",
        "      for line in file:\n",
        "\n",
        "        words_in_line = re.findall(r'\\b[a-z]+\\b', line.lower())\n",
        "\n",
        "        if not words_in_line:\n",
        "          prev_word = None # Reset if line is empty or has no words\n",
        "          continue\n",
        "\n",
        "        for word in words_in_line:\n",
        "          unigram_counts[word] += 1\n",
        "          processed_words += 1\n",
        "          if prev_word is not None:\n",
        "              # Increment count for the bigram (prev_word, word)\n",
        "              bigram_counts[prev_word][word] += 1\n",
        "\n",
        "          prev_word = word # Update prev_word for the next iteration\n",
        "\n",
        "\n",
        "    total_words = sum(unigram_counts.values())\n",
        "    print(f\"Successfully processed the corpus for N-grams.\")\n",
        "    print(f\"Total words counted (unigrams): {total_words}\")\n",
        "    print(f\"Unique words in vocabulary: {len(unigram_counts)}\")\n",
        "    # Calculate total number of bigram pairs counted\n",
        "    total_bigrams = sum(sum(inner_counter.values()) for inner_counter in bigram_counts.values())\n",
        "    print(f\"Total bigram pairs counted: {total_bigrams}\")\n",
        "    # Calculate unique preceding words in bigrams\n",
        "    print(f\"Unique preceding words in bigrams: {len(bigram_counts)}\")\n",
        "\n",
        "\n",
        "  except FileNotFoundError:\n",
        "    print(f\"Error: File not found at '{filepath}'. Cannot process.\")\n",
        "    return None, None\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred during N-gram processing: {e}\")\n",
        "    return None, None\n",
        "\n",
        "  return unigram_counts, bigram_counts, total_words\n",
        "\n",
        "\n",
        "def save_ngram_data(data, pickle_path):\n",
        "  \"\"\"Saves (unigrams, bigrams, total_words) tuple to a pickle file.\"\"\"\n",
        "  print(f\"Saving N-gram data to {pickle_path}...\")\n",
        "  try:\n",
        "    with open(pickle_path, 'wb') as file:\n",
        "      pickle.dump(data, file)\n",
        "    print(\"N-gram data saved successfully.\")\n",
        "  except Exception as e:\n",
        "    print(f\"Error saving N-gram data: {e}\")\n",
        "\n",
        "def load_ngram_data(pickle_path):\n",
        "  \"\"\"Loads (unigrams, bigrams, total_words) tuple from a pickle file.\"\"\"\n",
        "  print(f\"Attempting to load N-gram data from {pickle_path}...\")\n",
        "  try:\n",
        "    with open(pickle_path, 'rb') as file:\n",
        "      unigrams, bigrams, total_words = pickle.load(file)\n",
        "    print(\"N-gram data loaded successfully.\")\n",
        "    print(f\"Unique words loaded: {len(unigrams)}\")\n",
        "    print(f\"Unique preceding words loaded: {len(bigrams)}\")\n",
        "    print(f\"Total word count loaded: {total_words}\")\n",
        "    return unigrams, bigrams, total_words\n",
        "  except FileNotFoundError:\n",
        "    print(\"N-gram pickle file not found.\")\n",
        "    return None, None, None\n",
        "  except Exception as e:\n",
        "    print(f\"Error loading N-gram data: {e}\")\n",
        "    return None, None, None\n",
        "\n",
        "\n",
        "ngram_pickle_filepath = '/kaggle/working/ngram_data.pkl'\n",
        "\n",
        "previous_output_ngram_path = '/kaggle/input/spelling/ngram_data.pkl'\n",
        "UNI_COUNTS, BI_COUNTS, TOTAL_WORDS = None, None, None\n",
        "\n",
        "# List of paths to try loading from\n",
        "load_paths_ngram = [previous_output_ngram_path, ngram_pickle_filepath]\n",
        "\n",
        "# 1. Try loading from potential previous output locations\n",
        "found_data = False\n",
        "for path in load_paths_ngram:\n",
        "    if os.path.exists(path):\n",
        "        UNI_COUNTS, BI_COUNTS, TOTAL_WORDS = load_ngram_data(path)\n",
        "        if UNI_COUNTS is not None: # Check if loading was truly successful\n",
        "            found_data = True\n",
        "            break # Stop searching once loaded\n",
        "\n",
        "# 2. If loading failed, process the original corpus\n",
        "if not found_data:\n",
        "  print(\"\\n--- Loading Failed: Processing Corpus for N-grams ---\")\n",
        "  if not os.path.exists(corpus_filepath):\n",
        "      print(f\"ERROR: Corpus file not found at {corpus_filepath}. Please check the path.\")\n",
        "  else:\n",
        "      UNI_COUNTS, BI_COUNTS, TOTAL_WORDS = process_corpus_ngrams(corpus_filepath)\n",
        "\n",
        "  # 3. If processing was successful, save the results\n",
        "  if UNI_COUNTS and BI_COUNTS:\n",
        "    os.makedirs('/kaggle/working', exist_ok=True)  # Ensure directory exists\n",
        "    save_ngram_data((UNI_COUNTS, BI_COUNTS, TOTAL_WORDS), ngram_pickle_filepath)\n",
        "\n",
        "  else:\n",
        "    print(\"Failed to process the corpus for N-grams. Cannot save.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YULhVMADn52h",
        "outputId": "0fdb25ff-2c04-4b7f-8c87-ffc24a67dcb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Loading Failed: Processing Corpus for N-grams ---\n",
            "Attempting to read corpus file for N-grams: /kaggle/input/spelling/big.txt\n",
            "Successfully processed the corpus for N-grams.\n",
            "Total words counted (unigrams): 1100811\n",
            "Unique words in vocabulary: 29056\n",
            "Total bigram pairs counted: 1079057\n",
            "Unique preceding words in bigrams: 28730\n",
            "Saving N-gram data to /kaggle/working/ngram_data.pkl...\n",
            "N-gram data saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating bigram probabilities\n",
        "def P_unigram(word, unigram_counts, total_words, vocab_size):\n",
        "  \"\"\"Calculates smoothed Unigram probability P(word).\"\"\"\n",
        "  # Add-1 Smoothing\n",
        "  count = unigram_counts.get(word, 0)\n",
        "  return (count + 1) / (total_words + vocab_size)\n",
        "\n",
        "def P_bigram(word, prev_word, unigram_counts, bigram_counts, vocab_size):\n",
        "  \"\"\"Calculates smoothed Bigram probability P(word | prev_word).\"\"\"\n",
        "  bigram_count = bigram_counts.get(prev_word, Counter()).get(word, 0)\n",
        "  prev_unigram_count = unigram_counts.get(prev_word, 0)\n",
        "\n",
        "  # V is the size of the vocabulary\n",
        "  return (bigram_count + 1) / (prev_unigram_count + vocab_size)\n",
        "\n",
        "\n",
        "VOCAB_SIZE = 0\n",
        "if UNI_COUNTS:\n",
        "    VOCAB_SIZE = len(UNI_COUNTS)\n",
        "    print(f\"Vocabulary Size (for smoothing): {VOCAB_SIZE}\")\n",
        "\n",
        "\n",
        "\n",
        "from math import log\n",
        "\n",
        "def suggest_correction_ngram_comparative(word_to_correct, prev_word, next_word,\n",
        "                                        unigram_counts, bigram_counts, total_words, vocab_size):\n",
        "    \"\"\"\n",
        "    Spelling correction optimized for big.txt corpus, using context-aware n-gram scoring.\n",
        "    \"\"\"\n",
        "    original_word = word_to_correct.lower()\n",
        "\n",
        "    # 1. Candidate generation (similar to Norvig's but with context awareness)\n",
        "    candidates = (known([original_word], unigram_counts) or\n",
        "                 known(edits1(original_word), unigram_counts) or\n",
        "                 known(edits2(original_word), unigram_counts) or\n",
        "                 [original_word])\n",
        "\n",
        "    # 2. Scoring with context (optimized for big.txt patterns)\n",
        "    def get_score(candidate):\n",
        "        score = 0.0\n",
        "\n",
        "        # Base unigram score (log probability)\n",
        "        unigram_prob = P_unigram(candidate, unigram_counts, total_words, vocab_size)\n",
        "        score += log(unigram_prob + 1e-10)\n",
        "\n",
        "        # Previous context bonus\n",
        "        if prev_word and (prev_word, candidate) in bigram_counts:\n",
        "            bigram_prob = P_bigram(candidate, prev_word, unigram_counts, bigram_counts, vocab_size)\n",
        "            score += log(bigram_prob + 1e-10) * 1.5  # Weight context more heavily\n",
        "\n",
        "        # Next context bonus\n",
        "        if next_word and (candidate, next_word) in bigram_counts:\n",
        "            bigram_prob = P_bigram(next_word, candidate, unigram_counts, bigram_counts, vocab_size)\n",
        "            score += log(bigram_prob + 1e-10) * 1.5\n",
        "\n",
        "        return score\n",
        "\n",
        "    # 3. Select best candidate with Norvig-inspired thresholds\n",
        "    best_candidate = max(candidates, key=get_score)\n",
        "    best_score = get_score(best_candidate)\n",
        "\n",
        "    original_score = get_score(original_word) if original_word in candidates else -float('inf')\n",
        "\n",
        "    # Only correct if significant improvement\n",
        "    if (best_candidate != original_word and\n",
        "        (best_score - original_score) > log(20)):\n",
        "        return best_candidate\n",
        "\n",
        "    return original_word\n",
        "\n",
        "def correct_sentence_ngram_comparative(sentence, unigram_counts, bigram_counts, total_words, vocab_size):\n",
        "    \"\"\"Sentence correction optimized for big.txt content patterns.\"\"\"\n",
        "    tokens = tokenize_sentence(sentence)\n",
        "    corrected_tokens = []\n",
        "\n",
        "    for i, token in enumerate(tokens):\n",
        "        if not re.match(r'[a-zA-Z\\']+', token):\n",
        "            corrected_tokens.append(token)\n",
        "            continue\n",
        "\n",
        "        original_case = token\n",
        "        original_word = token.lower()\n",
        "\n",
        "        # Get context words (skip non-words)\n",
        "        prev_word = corrected_tokens[-1].lower() if (corrected_tokens and\n",
        "                          re.match(r'[a-zA-Z\\']+', corrected_tokens[-1])) else None\n",
        "        next_word = tokens[i+1].lower() if (i+1 < len(tokens) and\n",
        "                          re.match(r'[a-zA-Z\\']+', tokens[i+1])) else None\n",
        "\n",
        "        correction = suggest_correction_ngram_comparative(\n",
        "            original_word, prev_word, next_word,\n",
        "            unigram_counts, bigram_counts, total_words, vocab_size\n",
        "        )\n",
        "\n",
        "        # Case restoration\n",
        "        if original_case.istitle():\n",
        "            correction = correction.title()\n",
        "        elif original_case.isupper():\n",
        "            correction = correction.upper()\n",
        "        elif \"'\" in original_case:  # Handle \"I'M\" -> \"I'm\" etc.\n",
        "            parts = original_case.split(\"'\")\n",
        "            if len(parts) == 2 and len(parts[1]) > 0:\n",
        "                correction = correction.replace(\"'\", \"'\").replace(\n",
        "                    correction.split(\"'\")[1].lower(), parts[1].lower())\n",
        "\n",
        "        corrected_tokens.append(correction)\n",
        "\n",
        "    return \"\".join(corrected_tokens)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\" and 'UNI_COUNTS' in globals() and 'BI_COUNTS' in globals():\n",
        "    print(\"\\n--- Testing with big.txt-optimized Correction ---\")\n",
        "\n",
        "    # These examples reflect common errors that should be correctable with big.txt content\n",
        "    test_sentences = [\n",
        "        \"I can not beleive it.\",\n",
        "        \"You cay thaaat there was a gentleman.\",\n",
        "        \"Their is a problem here.\",\n",
        "        \"She recieved the letter.\",\n",
        "        \"Its all about the context.\",\n",
        "        \"The goverment announced plans.\",\n",
        "        \"He completly forgot.\",\n",
        "        \"They where happy together.\",\n",
        "        \"This is more prefered.\",\n",
        "        \"An artifical intelligence.\",\n",
        "        \"Tha definate answer.\",\n",
        "        \"A seperate occasion.\",\n",
        "        \"hiz eyes are beautifol.\",\n",
        "    ]\n",
        "\n",
        "    for sentence in test_sentences:\n",
        "        corrected = correct_sentence_ngram_comparative(\n",
        "            sentence, UNI_COUNTS, BI_COUNTS, TOTAL_WORDS, VOCAB_SIZE\n",
        "        )\n",
        "        print(f\"Original:  '{sentence}'\")\n",
        "        print(f\"Corrected: '{corrected}'\")\n",
        "        print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qI6j4EYq8PR",
        "outputId": "676255a8-3eb7-46e1-e605-69e8f9459561"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size (for smoothing): 29056\n",
            "\n",
            "--- Testing with big.txt-optimized Correction ---\n",
            "Original:  'I can not beleive it.'\n",
            "Corrected: 'I can not believe it.'\n",
            "--------------------------------------------------\n",
            "Original:  'You cay thaaat there was a gentleman.'\n",
            "Corrected: 'You may that there was a gentleman.'\n",
            "--------------------------------------------------\n",
            "Original:  'Their is a problem here.'\n",
            "Corrected: 'Their is a problem here.'\n",
            "--------------------------------------------------\n",
            "Original:  'She recieved the letter.'\n",
            "Corrected: 'She received the letter.'\n",
            "--------------------------------------------------\n",
            "Original:  'Its all about the context.'\n",
            "Corrected: 'Its all about the context.'\n",
            "--------------------------------------------------\n",
            "Original:  'The goverment announced plans.'\n",
            "Corrected: 'The government announced plans.'\n",
            "--------------------------------------------------\n",
            "Original:  'He completly forgot.'\n",
            "Corrected: 'He completely forgot.'\n",
            "--------------------------------------------------\n",
            "Original:  'They where happy together.'\n",
            "Corrected: 'They where happy together.'\n",
            "--------------------------------------------------\n",
            "Original:  'This is more prefered.'\n",
            "Corrected: 'This is more preferred.'\n",
            "--------------------------------------------------\n",
            "Original:  'An artifical intelligence.'\n",
            "Corrected: 'An artificial intelligence.'\n",
            "--------------------------------------------------\n",
            "Original:  'Tha definate answer.'\n",
            "Corrected: 'The definite answer.'\n",
            "--------------------------------------------------\n",
            "Original:  'A seperate occasion.'\n",
            "Corrected: 'A separate occasion.'\n",
            "--------------------------------------------------\n",
            "Original:  'hiz eyes are beautifol.'\n",
            "Corrected: 'his eyes are beautiful.'\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GUI\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "# --- GUI Widgets ---\n",
        "input_text = widgets.Textarea(\n",
        "    value=\"Their is a probleem with this sentance.\",\n",
        "    placeholder='Type a sentence with spelling mistakes',\n",
        "    description='Input:',\n",
        "    layout={'width': '600px', 'height': '100px'}\n",
        ")\n",
        "\n",
        "output_text = widgets.Textarea(\n",
        "    value=\"\",\n",
        "    description='Corrected:',\n",
        "    disabled=True,\n",
        "    layout={'width': '600px', 'height': '100px'}\n",
        ")\n",
        "\n",
        "button = widgets.Button(description=\"Correct Spelling\")\n",
        "\n",
        "def on_button_click(b):\n",
        "    corrected = correct_sentence_ngram_comparative(\n",
        "        input_text.value,\n",
        "        UNI_COUNTS,\n",
        "        BI_COUNTS,\n",
        "        TOTAL_WORDS,\n",
        "        VOCAB_SIZE\n",
        "    )\n",
        "    output_text.value = corrected\n",
        "\n",
        "button.on_click(on_button_click)\n",
        "\n",
        "display(widgets.VBox([input_text, button, output_text]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257,
          "referenced_widgets": [
            "5de816edd5f34960ae9ad653c88fdca0",
            "eca2afd7e5ed4b59b136f81425f766dc",
            "18b3b23ba05442489df167e9f9381a7d",
            "2634f29b2c2e446ab5c91cf5f18749a7",
            "15387d3cd94845eb909af2b2c34a9480",
            "cdfc5fbe58634c9184a053a493cebab4",
            "027fe51722fa4afea7c581a2fba36d3d",
            "94398afe99094efeb721ef368428530b",
            "b6f774c61dcb4317ab0a10a79b97284c",
            "0177cd24c466442e8c30cd34ce8ecc42",
            "0236dea914e84ec1909dda0f6b190488"
          ]
        },
        "id": "Cg9EiiWsq8Sn",
        "outputId": "0cb4de00-ae69-4e7f-d7fb-6a6325c3abe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Textarea(value='Their is a probleem with this sentance.', description='Input:', layout=Layout(h…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5de816edd5f34960ae9ad653c88fdca0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RTm94O-QybeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testfile_filepath='/kaggle/input/spelling/spell-testset1.txt'\n",
        "\n",
        "test_data = {}\n",
        "\n",
        "with open(testfile_filepath, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        correct, misspellings = line.split(':')\n",
        "        correct = correct.strip()\n",
        "        misspellings_list = misspellings.strip().split()\n",
        "        test_data[correct] = misspellings_list\n",
        "\n",
        "import re\n",
        "\n",
        "def tokenize(text):\n",
        "    \"\"\"Simple tokenizer to split into words, preserving apostrophes.\"\"\"\n",
        "    return re.findall(r\"[a-zA-Z']+\", text.lower())\n",
        "\n",
        "def evaluate_corrections(test_data, unigram_counts, bigram_counts, total_words, vocab_size):\n",
        "    TP, FP, FN, TN = 0, 0, 0, 0\n",
        "\n",
        "    for correct_word, misspellings in test_data.items():\n",
        "        for original_word in misspellings:\n",
        "            corrected_word = correct_sentence_ngram_comparative(\n",
        "                original_word, unigram_counts, bigram_counts, total_words, vocab_size\n",
        "            )\n",
        "\n",
        "            orig_tokens = tokenize(original_word)\n",
        "            corr_tokens = tokenize(corrected_word)\n",
        "            true_tokens = tokenize(correct_word)\n",
        "\n",
        "            for orig_word, corr_word, true_word in zip(orig_tokens, corr_tokens, true_tokens):\n",
        "                if orig_word != true_word:  # word misspelled originally\n",
        "                    if corr_word == true_word:  # corrected correctly\n",
        "                        TP += 1\n",
        "                    else:\n",
        "                        FN += 1\n",
        "                else:  # word was correct originally\n",
        "                    if corr_word == true_word:\n",
        "                        TN += 1\n",
        "                    else:\n",
        "                        FP += 1\n",
        "\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "    accuracy = (TP + TN) / (TP + FP + FN + TN) if (TP + FP + FN + TN) > 0 else 0\n",
        "\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "\n",
        "    return precision, recall, accuracy\n",
        "\n",
        "precision, recall, accuracy = evaluate_corrections(\n",
        "    test_data, UNI_COUNTS, BI_COUNTS, TOTAL_WORDS, VOCAB_SIZE\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKeO9LICyCmF",
        "outputId": "5519148a-19de-4efa-9264-ff371917d121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.0000\n",
            "Recall:    0.7481\n",
            "Accuracy:  0.7481\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NyToI0N2zXUL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}